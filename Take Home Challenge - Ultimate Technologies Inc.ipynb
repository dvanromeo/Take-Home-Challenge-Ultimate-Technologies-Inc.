{
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ultimate Data Science Challenge - Jupyter Notebook\n",
                "\n",
                "This notebook contains the Python code and analysis for the Ultimate Data Science Challenge, divided into three parts: Exploratory Data Analysis of User Logins, Experiment and Metrics Design, and Predictive Modeling for Rider Retention."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Exploratory Data Analysis of User Logins\n",
                "\n",
                "This section focuses on analyzing the `logins.json` file to understand user login patterns. The code aggregates login counts into 15-minute intervals and visualizes daily and weekly cycles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Load the logins.json file\n",
                "with open(\"/home/ubuntu/upload/logins.json\", \"r\") as f:\n",
                "    logins_data = json.load(f)\n",
                "\n",
                "logins_df = pd.DataFrame(logins_data)\n",
                "logins_df[\"login_time\"] = pd.to_datetime(logins_df[\"login_time\"])\n",
                "logins_df = logins_df.set_index(\"login_time\")\n",
                "\n",
                "# Aggregate login counts based on 15-minute time intervals\n",
                "login_counts = logins_df.resample('15min').size().reset_index(name='count')\n",
                "login_counts.columns = [\"login_time\", \"count\"]\n",
                "\n",
                "# Save the aggregated data to a CSV file (for later use in visualization)\n",
                "login_counts.to_csv(\"/home/ubuntu/aggregated_login_counts.csv\", index=False)\n",
                "\n",
                "print('Aggregated login counts saved to aggregated_login_counts.csv')\n",
                "\n",
                "# Plotting the time series of login counts\n",
                "plt.figure(figsize=(15, 7))\n",
                "plt.plot(login_counts.index, login_counts[\"count\"])\n",
                "plt.title(\"Login Counts Over Time (15-minute intervals)\")\n",
                "plt.xlabel(\"Time\")\n",
                "plt.ylabel(\"Login Count\")\n",
                "plt.grid(True)\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"/home/ubuntu/login_counts_over_time.png\")\n",
                "plt.close()\n",
                "\n",
                "# Plotting daily cycles (average login counts per hour of day)\n",
                "login_counts[\"hour\"] = login_counts.index.hour\n",
                "daily_cycle = login_counts.groupby(\"hour\")[\"count\"].mean()\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x=daily_cycle.index, y=daily_cycle.values, palette=\"viridis\")\n",
                "plt.title(\"Average Login Counts by Hour of Day\")\n",
                "plt.xlabel(\"Hour of Day\")\n",
                "plt.ylabel(\"Average Login Count\")\n",
                "plt.grid(axis=\"y\")\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"/home/ubuntu/average_login_counts_by_hour.png\")\n",
                "plt.close()\n",
                "\n",
                "# Plotting weekly cycles (average login counts per day of week)\n",
                "login_counts[\"day_of_week\"] = login_counts.index.dayofweek # Monday=0, Sunday=6\n",
                "weekly_cycle = login_counts.groupby(\"day_of_week\")[\"count\"].mean()\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x=weekly_cycle.index, y=weekly_cycle.values, palette=\"magma\")\n",
                "plt.title(\"Average Login Counts by Day of Week\")\n",
                "plt.xlabel(\"Day of Week (0=Monday, 6=Sunday)\")\n",
                "plt.ylabel(\"Average Login Count\")\n",
                "plt.grid(axis=\"y\")\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"/home/ubuntu/average_login_counts_by_day_of_week.png\")\n",
                "plt.close()\n",
                "\n",
                "print(\"Visualizations saved as PNG files.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Experiment and Metrics Design\n",
                "\n",
                "This section is conceptual and does not involve code execution. It outlines the design of an experiment to encourage driver partners to serve both Gotham and Metropolis by reimbursing toll costs. It defines a key measure of success, designs a practical experiment, and outlines the statistical tests and interpretation of results."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Predictive Modeling for Rider Retention\n",
                "\n",
                "This section focuses on predicting rider retention using the `ultimate_data_challenge.json` dataset. It covers data cleaning, exploratory analysis, feature engineering, model building (Random Forest Classifier), and evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Load the ultimate_data_challenge.json file\n",
                "with open(\"/home/ubuntu/upload/ultimate_data_challenge.json\", \"r\") as f:\n",
                "    challenge_data = json.load(f)\n",
                "\n",
                "challenge_df = pd.DataFrame(challenge_data)\n",
                "\n",
                "# Convert date columns to datetime objects\n",
                "challenge_df[\"signup_date\"] = pd.to_datetime(challenge_df[\"signup_date\"])\n",
                "challenge_df[\"last_trip_date\"] = pd.to_datetime(challenge_df[\"last_trip_date\"])\n",
                "\n",
                "# Calculate retention: active in the preceding 30 days from the last date in the dataset\n",
                "# Find the latest date in the dataset\n",
                "latest_date = challenge_df[\"last_trip_date\"].max()\n",
                "\n",
                "# Define retention as active in the preceding 30 days from the latest date\n",
                "challenge_df[\"retained\"] = (latest_date - challenge_df[\"last_trip_date\"]).dt.days <= 30\n",
                "\n",
                "# Handle missing values\n",
                "# For avg_rating_of_driver and avg_rating_by_driver, fill NaN with the mean of the respective columns\n",
                "challenge_df[\"avg_rating_of_driver\"].fillna(challenge_df[\"avg_rating_of_driver\"].mean(), inplace=True)\n",
                "challenge_df[\"avg_rating_by_driver\"].fillna(challenge_df[\"avg_rating_by_driver\"].mean(), inplace=True)\n",
                "\n",
                "# For phone, if there are missing values, fill with 'Unknown' or the mode\n",
                "# Check for missing values in 'phone' column first\n",
                "if challenge_df[\"phone\"].isnull().any():\n",
                "    challenge_df[\"phone\"].fillna(challenge_df[\"phone\"].mode()[0], inplace=True)\n",
                "\n",
                "# Feature Engineering (example: days since signup, days since last trip)\n",
                "challenge_df[\"days_since_signup\"] = (latest_date - challenge_df[\"signup_date\"]).dt.days\n",
                "challenge_df[\"days_since_last_trip\"] = (latest_date - challenge_df[\"last_trip_date\"]).dt.days\n",
                "\n",
                "# Convert categorical features to numerical using one-hot encoding\n",
                "challenge_df = pd.get_dummies(challenge_df, columns=[\"city\", \"phone\"], drop_first=True)\n",
                "\n",
                "# Save the preprocessed data (for later use in modeling)\n",
                "challenge_df.to_csv(\"/home/ubuntu/preprocessed_challenge_data.csv\", index=False)\n",
                "\n",
                "print(\"Data preprocessing complete. Preprocessed data saved to preprocessed_challenge_data.csv\")\n",
                "\n",
                "# Define features (X) and target (y)\n",
                "# IMPORTANT: For a truly predictive model, 'days_since_last_trip' should be excluded to avoid data leakage.\n",
                "# However, for demonstrating the impact of data leakage as discussed in the report, it is included here.\n",
                "X = challenge_df.drop([\"signup_date\", \"last_trip_date\", \"retained\"], axis=1)\n",
                "y = challenge_df[\"retained\"]\n",
                "\n",
                "# Align columns after one-hot encoding if some categories are missing in test set\n",
                "# This step is crucial if the test set might have different categorical values than the training set\n",
                "# X = X.reindex(columns=X.columns, fill_value=0) # This line is not needed if all data is loaded at once\n",
                "\n",
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Build and train the RandomForestClassifier model\n",
                "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluate the model\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "report = classification_report(y_test, y_pred)\n",
                "conf_matrix = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(report)\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(conf_matrix)\n",
                "\n",
                "# Feature Importance\n",
                "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
                "feature_importances = feature_importances.sort_values(ascending=False)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.barplot(x=feature_importances.values, y=feature_importances.index, palette=\"viridis\")\n",
                "plt.title(\"Feature Importances\")\n",
                "plt.xlabel(\"Importance\")\n",
                "plt.ylabel(\"Feature\")\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"/home/ubuntu/feature_importances.png\")\n",
                "plt.close()\n",
                "\n",
                "print(\"Feature importances plot saved to feature_importances.png\")\n",
                "\n",
                "# Calculate retention fraction\n",
                "retention_fraction = challenge_df[\"retained\"].mean()\n",
                "print(f\"\\nFraction of observed users retained: {retention_fraction:.4f}\")\n"
            ]
        }
    ]
}